{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis\n",
        "This is a notebook containing Sentiment Analysis Mini Project on Amazon Musical Instruments Reviews. I am interested in Natural Language Processing and that is my motivation to make this project. I think that sentiment analysis has a really powerful impacts in business developments because we can gain so many insights from here."
      ],
      "metadata": {
        "id": "BuHmnUK8bN6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries\n"
      ],
      "metadata": {
        "id": "ddcGBikmbRxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4PLJq_YEbTY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Text Libraries"
      ],
      "metadata": {
        "id": "Yt_3FPWZbiER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "seI85Z8XbhRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA Analysis"
      ],
      "metadata": {
        "id": "dmr--MnDboAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Text Polarity\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Text Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Word Cloud\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "y-e2fUsjbrQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "hsR369pTbvQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Resampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Splitting Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FqvTqLKwbytz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection and Evaluation"
      ],
      "metadata": {
        "id": "gImHly1Ob027"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model Building\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Model Metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "lRJTG0Xab3j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Dataset\n",
        "The dataset that we will use is taken from Kaggle website and can be downloaded here:\n",
        "\n",
        "Amazon Musical Instruments Reviews\n",
        "\n",
        "There are two formats available of the dataset: JSON and CSV. We will use the CSV one in this project.\n",
        "\n",
        "Overall, the dataset talks about the feedback received after the customers purchased musical instruments from Amazon."
      ],
      "metadata": {
        "id": "Ebcav9Lnb7Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read The Dataset"
      ],
      "metadata": {
        "id": "CSR_2IsFb9cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = pd.read_csv(\"/content/Instruments_Reviews.csv\")"
      ],
      "metadata": {
        "id": "yMzJfVGdb_18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shape of The Dataset"
      ],
      "metadata": {
        "id": "9ITXR0j1cGgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "Ye8mdP6LcMOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we can infer that the dataset consists of 10261 rows and 9 columns."
      ],
      "metadata": {
        "id": "Zfjkj8pNcPi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n",
        "Checking Null Values"
      ],
      "metadata": {
        "id": "Tk7R2CfIcRoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "wDN_yz4UcT1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above, there are two columns in the dataset with null values: reviewText and reviewerName. While the latter one is not really important, we should focus on the first column. We cannot remove these rows because the ratings and summary given from the customers will have some effects to our model later (although the number of missing rows is small). Because of it, we can fill the empty values with an empty string."
      ],
      "metadata": {
        "id": "GNx8unqccWol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Filling Missing Values"
      ],
      "metadata": {
        "id": "PfvUh3sdce29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.reviewText.fillna(value = \"\", inplace = True)"
      ],
      "metadata": {
        "id": "aWZPo8jUchL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Concatenate reviewText and summary Columns"
      ],
      "metadata": {
        "id": "OOF8GWErckfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"reviews\"] = dataset[\"reviewText\"] + \" \" + dataset[\"summary\"]\n",
        "dataset.drop(columns = [\"reviewText\", \"summary\"], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "9zXugCbNcmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Statistic Description of The Dataset"
      ],
      "metadata": {
        "id": "DCIlVCQecp5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.describe(include = \"all\")"
      ],
      "metadata": {
        "id": "I99afAyUcmT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the description above, we know that the ratings given from the customers will have the range of [1, 5] as shown above. Also, the average rating given to musical instruments sold is 4.48. We can also see our new column reviews is there to concate both summary and reviewText."
      ],
      "metadata": {
        "id": "TPVP05uAcx0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Percentages of Ratings Given from The Customers"
      ],
      "metadata": {
        "id": "DcoR1z-8c13c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.overall.value_counts().plot(kind = \"pie\", legend = False, autopct = \"%1.2f%%\", fontsize = 10, figsize=(8,8))\n",
        "plt.title(\"Percentages of Ratings Given from The Customers\", loc = \"center\")\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "pRGEfYnwcmao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart above, the majority of musical instruments sold on Amazon have perfect ratings of 5.0, meaning the condition of the products are good. If we were to denote that ratings above 3 are positive, ratings equal to 3 are neutral, and ratings under 3 are negative, we know that the number of negative reviews given in the dataset are relatively small. This might affect our model later."
      ],
      "metadata": {
        "id": "Ix2e7ia6c8UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Labelling Products Based On Ratings Given\n",
        "\n",
        "Our dataset does not have any dependent variable, or in other words we haven't had any prediction target yet. We will categorize each sentiment according to ratings given for each row based on the explanation before: Positive Label for products with rating bigger than 3.0, Neutral Label for products with rating equal to 3.0, else Negative Label."
      ],
      "metadata": {
        "id": "_KeZoqLkc99e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Labelling(Rows):\n",
        "  if(Rows[\"overall\"] > 3.0):\n",
        "    Label = \"Positive\"\n",
        "  elif(Rows[\"overall\"] < 3.0):\n",
        "    Label = \"Negative\"\n",
        "  else:\n",
        "    Label = \"Neutral\"\n",
        "  return Label"
      ],
      "metadata": {
        "id": "9fiHr6a4cmc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"sentiment\"] = dataset.apply(Labelling, axis = 1)\n",
        ""
      ],
      "metadata": {
        "id": "hWMqVCO_dBU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"sentiment\"].value_counts().plot(kind = \"bar\", color = \"blue\")\n",
        "plt.title(\"Amount of Each Sentiments Based On Rating Given\", loc = \"center\", fontsize = 15, color = \"red\", pad = 25)\n",
        "plt.xlabel(\"Sentiments\", color = \"green\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Amount of Sentiments\", color = \"green\", fontsize = 10, labelpad = 15)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "OAk8ehQbdBX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we can actually change the labels into numeric values but for the sake of experiments we will do it later. Also, notice that from the graph we can know that most of our data contains positive sentiments, which is true from the exploration before."
      ],
      "metadata": {
        "id": "tTjTJSfXdIXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Preprocessing\n",
        "Text Cleaning"
      ],
      "metadata": {
        "id": "5vojHZDYdLAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Text_Cleaning(Text):\n",
        "  # Lowercase the texts\n",
        "  Text = Text.lower()\n",
        "\n",
        "  # Cleaning punctuations in the text\n",
        "  punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  Text = Text.translate(punc)\n",
        "\n",
        "  # Removing numbers in the text\n",
        "  Text = re.sub(r'\\d+', '', Text)\n",
        "\n",
        "  # Remove possible links\n",
        "  Text = re.sub('https?://\\S+|www\\.\\S+', '', Text)\n",
        "\n",
        "  # Deleting newlines\n",
        "  Text = re.sub('\\n', '', Text)\n",
        "\n",
        "  return Text"
      ],
      "metadata": {
        "id": "FZrxyWE4dBaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Processing"
      ],
      "metadata": {
        "id": "NLt4fG2pdRyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stopwords\n",
        "Stopwords = set(nltk.corpus.stopwords.words(\"english\")) - set([\"not\"])\n",
        "\n",
        "def Text_Processing(Text):\n",
        "  Processed_Text = list()\n",
        "  Lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Tokens of Words\n",
        "  Tokens = nltk.word_tokenize(Text)\n",
        "\n",
        "  # Removing Stopwords and Lemmatizing Words\n",
        "  # To reduce noises in our dataset, also to keep it simple and still\n",
        "  # powerful, we will only omit the word `not` from the list of stopwords\n",
        "\n",
        "  for word in Tokens:\n",
        "    if word not in Stopwords:\n",
        "      Processed_Text.append(Lemmatizer.lemmatize(word))\n",
        "\n",
        "  return(\" \".join(Processed_Text))"
      ],
      "metadata": {
        "id": "M_iAT_rFdBdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying The Functions"
      ],
      "metadata": {
        "id": "ImXp2wcodW_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"reviews\"] = dataset[\"reviews\"].apply(lambda Text: Text_Cleaning(Text))\n",
        "dataset[\"reviews\"] = dataset[\"reviews\"].apply(lambda Text: Text_Processing(Text))"
      ],
      "metadata": {
        "id": "sViwYKgpdBgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis\n",
        "Overview of The Dataset"
      ],
      "metadata": {
        "id": "KwU95GcAddU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.head(n = 10)"
      ],
      "metadata": {
        "id": "dCD6QTDfdBjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the overview above, we know that for sentiment analysis that we will do, reviews is important to our model and we should use this aspect as our feature. By using this feature, we will need to predict what our sentiment will be classified into."
      ],
      "metadata": {
        "id": "w_05CtnHdjjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#About Other Features"
      ],
      "metadata": {
        "id": "GqU3YocfdlaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.describe(include = \"all\")"
      ],
      "metadata": {
        "id": "UGwm786wdnc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will go back to statistic description of our dataset. Intuitively, the other features from our dataset does not really have any impact in determining our sentiment later. We might use the helpful part in our model, but as we can see from the description above, the top values of it is [0,0], which means that most users do not really take their votes in it. Because of it, we can also decide that we don't really need it in our model."
      ],
      "metadata": {
        "id": "03vIe1V9drwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Polarity, Review Length, and Word Counts\n",
        "\n",
        "To justify our analysis before, we will dive further into the dataset a bit more from the polarity of the texts, also from the words used in the reviews. We will generate some new columns in our dataset and visualize it.\n",
        "\n",
        "Polarity"
      ],
      "metadata": {
        "id": "6MCXM7bCdvWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "id": "350sjfhiePwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "NoVlVdoKeX2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"polarity\"] = dataset[\"reviews\"].map(lambda Text: TextBlob(Text).sentiment.polarity)"
      ],
      "metadata": {
        "id": "v5QY_Y0ceApS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"polarity\"].plot(kind = \"hist\", bins = 40, edgecolor = \"blue\", linewidth = 1, color = \"orange\", figsize = (10,5))\n",
        "plt.title(\"Polarity Score in Reviews\", color = \"blue\", pad = 20)\n",
        "plt.xlabel(\"Polarity\", labelpad = 15, color = \"red\")\n",
        "plt.ylabel(\"Amount of Reviews\", labelpad = 20, color = \"green\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yIwm9GdjecRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews with negative polarity will be in range of [-1, 0), neutral ones will be 0.0, and positive reviews will have the range of (0, 1].\n",
        "\n",
        "From the histogram above, we know that most of the reviews are distributed in positive sentiments, meaning that what we extracted from our analysis before is true. Statistically, this histogram shows that our data is normally distributed, but not with standard distribution. In conclusion, we know for sure that our analysis about the amount of sentiments from the reviews is correct and corresponds to the histogram above.\n",
        "\n",
        "Review Length"
      ],
      "metadata": {
        "id": "AmZ4JTDLeiqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"length\"] = dataset[\"reviews\"].astype(str).apply(len)"
      ],
      "metadata": {
        "id": "aT8K72G9emqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"length\"].plot(kind = \"hist\", bins = 40, edgecolor = \"blue\", linewidth = 1, color = \"orange\", figsize = (10,5))\n",
        "plt.title(\"Length of Reviews\", color = \"blue\", pad = 20)\n",
        "plt.xlabel(\"Length\", labelpad = 15, color = \"red\")\n",
        "plt.ylabel(\"Amount of Reviews\", labelpad = 20, color = \"green\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9r42bX79eorl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this, we know that our review has text length between approximately 0-1000 characters. The distribution itself has positive skewness, or in other words it is skewed right, and this means that our reviews rarely has larger length than 1000 characters. Of course, the review that we use here is affected by the text preprocessing phase, so the length might not be the actual value of the review itself as some words might have been omitted already. This will also have the same effect when we count the tatal of words in our reviews."
      ],
      "metadata": {
        "id": "3_ltN79yeury"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Counts"
      ],
      "metadata": {
        "id": "hE591HswewRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dataset[\"word_counts\"] = dataset[\"reviews\"].apply(lambda x: len(str(x).split()))\n",
        ""
      ],
      "metadata": {
        "id": "9uv-_3NBesJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"word_counts\"].plot(kind = \"hist\", bins = 40, edgecolor = \"blue\", linewidth = 1, color = \"orange\", figsize = (10,5))\n",
        "plt.title(\"Word Counts in Reviews\", color = \"blue\", pad = 20)\n",
        "plt.xlabel(\"Word Counts\", labelpad = 15, color = \"red\")\n",
        "plt.ylabel(\"Amount of Reviews\", labelpad = 20, color = \"green\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5lXqmehjesNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the figure above, we infer that most of the reviews consist of 0-200 words. Just like before, the distribution is skewed right and the calculation is affected by our text preprocessing phase before."
      ],
      "metadata": {
        "id": "5NLsblkDe6B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#N-Gram Analysis\n",
        "N-Gram Function"
      ],
      "metadata": {
        "id": "1ODRPMyfe8OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Gram_Analysis(Corpus, Gram, N):\n",
        "  # Vectorizer\n",
        "  Vectorizer = CountVectorizer(stop_words = Stopwords, ngram_range=(Gram,Gram))\n",
        "\n",
        "  # N-Grams Matrix\n",
        "  ngrams = Vectorizer.fit_transform(Corpus)\n",
        "\n",
        "  # N-Grams Frequency\n",
        "  Count = ngrams.sum(axis=0)\n",
        "\n",
        "  # List of Words\n",
        "  words = [(word, Count[0, idx]) for word, idx in Vectorizer.vocabulary_.items()]\n",
        "\n",
        "  # Sort Descending With Key = Count\n",
        "  words = sorted(words, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "  return words[:N]\n",
        ""
      ],
      "metadata": {
        "id": "Fv6IPrMQe-f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter The DataFrame Based On Sentiments"
      ],
      "metadata": {
        "id": "thoDGTpWfBEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use dropna() so the base DataFrame is not affected\n",
        "Positive = dataset[dataset[\"sentiment\"] == \"Positive\"].dropna()\n",
        "Neutral = dataset[dataset[\"sentiment\"] == \"Neutral\"].dropna()\n",
        "Negative = dataset[dataset[\"sentiment\"] == \"Negative\"].dropna()\n",
        ""
      ],
      "metadata": {
        "id": "PwrGzaH_fDRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unigram of Reviews Based on Sentiments"
      ],
      "metadata": {
        "id": "QQ_F7IDjfJRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OCmijLJlfmuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define stop words as a list\n",
        "Stopwords = ['an', 'if', 'for', 'amongst', 'seeming', 'themselves', 'before', 'about', 'around', 'name',\n",
        "             'their', 'nine', 'last', 'everyone', 'thin', 'that', 'on', 'except', 'his', 'off', 'become',\n",
        "             'is', 'wherein', 'along', 'cannot', 'nothing', 'thereafter', 'etc', 'own', 'to', 'five', 'its',\n",
        "             'the', 'ever', 'interest', 'such', 'none', 'get', 'them', 'down', 'found', 'onto', 'not', 'me',\n",
        "             'both', 'almost', 'my', 'others', 'alone', 'twenty', 'between', 'somewhere', 'top', 'former',\n",
        "             'anyhow', 'no', 'most', 'bottom', 'becoming', 'whom', 'through', 'into', 'anyone', 'been', 'we',\n",
        "             'will', 'everywhere', 'whereas', 'indeed', 'but', 'again', 'behind', 'seems', 'forty', 'find',\n",
        "             'whether', 'at', 'any', 'mine', 'may', 'latter', 'so', 'below', 'describe', 'un', 'hereupon',\n",
        "             'anyway', 'have', 'enough', 'four', 'up', 'someone', 'noone', 'whereupon', 'her', 'had',\n",
        "             'wherever', 'another', 'towards', 'already', 'as', 'therein', 'yours', 'thus', 'keep', 'eleven',\n",
        "             'many', 'several', 'either', 'six', 'do', 'cry', 'during', 'now', 'what', 'whenever', 'toward',\n",
        "             'ltd', 'meanwhile', 'yourself', 'all', 'over', 'in', 'first', 'this', 'else', 'without', 'fifteen',\n",
        "             'you', 'being', 'go', 'thick', 'herein', 'well', 'whence', 'once', 'might', 'beforehand', 'mill',\n",
        "             'more', 'somehow', 'too', 'inc', 'because', 'itself', 'formerly', 'from', 'elsewhere', 'take', 'a']\n",
        "\n",
        "# Gram Analysis function\n",
        "def Gram_Analysis(Corpus, Gram, N):\n",
        "    # Vectorizer\n",
        "    Vectorizer = CountVectorizer(stop_words=Stopwords, ngram_range=(Gram, Gram))\n",
        "\n",
        "    # N-Grams Matrix\n",
        "    X = Vectorizer.fit_transform(Corpus)\n",
        "\n",
        "    # Sum of N-Grams\n",
        "    N_Grams = X.sum(axis=0)\n",
        "\n",
        "    # Extracting N-Grams\n",
        "    Words = [(word, N_Grams[0, idx]) for word, idx in Vectorizer.vocabulary_.items()]\n",
        "\n",
        "    # Sorting\n",
        "    Words = sorted(Words, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return Words[:N]\n",
        "\n",
        "# Finding Unigram\n",
        "words = Gram_Analysis(Neutral[\"reviews\"], 1, 20)\n",
        "Unigram = pd.DataFrame(words, columns=[\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Unigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind=\"barh\", color=\"orange\", figsize=(10, 5))\n",
        "plt.title(\"Unigram of Reviews with Neutral Sentiments\", loc=\"center\", fontsize=15, color=\"blue\", pad=25)\n",
        "plt.xlabel(\"Total Counts\", color=\"magenta\", fontsize=10, labelpad=15)\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylabel(\"Top Words\", color=\"cyan\", fontsize=10, labelpad=15)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f09_PvEsgOig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Unigram\n",
        "words = Gram_Analysis(Negative[\"reviews\"], 1, 20)\n",
        "Unigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Unigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"red\", figsize = (10, 5))\n",
        "plt.title(\"Unigram of Reviews with Negative Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "Uu6gGYOlgV0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These unigrams are not really accurate, because we can clearly see that even for postive sentiments, the top unigram is the wird guitar which is an object, though from here we might know that the most frequently bought items are guitars or the complement of it. We should try to find the bigram and see how accurate it can describe each sentiments"
      ],
      "metadata": {
        "id": "-RY9UXa_gas-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bigram of Reviews Based On Sentiments"
      ],
      "metadata": {
        "id": "WmvTZmmagg7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Bigram\n",
        "words = Gram_Analysis(Positive[\"reviews\"], 2, 20)\n",
        "Bigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Bigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"green\", figsize = (10, 5))\n",
        "plt.title(\"Bigram of Reviews with Positive Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RFcRqoEtgk4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Bigram\n",
        "words = Gram_Analysis(Neutral[\"reviews\"], 2, 20)\n",
        "Bigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Bigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"orange\", figsize = (10, 5))\n",
        "plt.title(\"Bigram of Reviews with Neutral Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "Mq2gysCTgoga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Bigram\n",
        "words = Gram_Analysis(Negative[\"reviews\"], 2, 20)\n",
        "Bigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Bigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"red\", figsize = (10, 5))\n",
        "plt.title(\"Bigram of Reviews with Negative Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4vzoKljrg2z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The bigrams work better than the unigrams, because we can actually see some phrases that really describe what a good sentiment is. Although, in some parts we can still see guitar objects as the top words, which make us believe that our interpretation about the most selling items are related to guitars."
      ],
      "metadata": {
        "id": "Vlq0f0aUlWP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trigram of Reviews Based On Sentiments"
      ],
      "metadata": {
        "id": "ck46ZA39laEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Trigram\n",
        "words = Gram_Analysis(Positive[\"reviews\"], 3, 20)\n",
        "Trigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Trigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"green\", figsize = (10, 5))\n",
        "plt.title(\"Trigram of Reviews with Positive Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwZO6yI3lcmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Trigram\n",
        "words = Gram_Analysis(Neutral[\"reviews\"], 3, 20)\n",
        "Trigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Trigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"orange\", figsize = (10, 5))\n",
        "plt.title(\"Trigram of Reviews with Neutral Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "4Mp1HLF5lf7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Trigram\n",
        "words = Gram_Analysis(Negative[\"reviews\"], 3, 20)\n",
        "Trigram = pd.DataFrame(words, columns = [\"Words\", \"Counts\"])\n",
        "\n",
        "# Visualization\n",
        "Trigram.groupby(\"Words\").sum()[\"Counts\"].sort_values().plot(kind = \"barh\", color = \"red\", figsize = (10, 5))\n",
        "plt.title(\"Trigram of Reviews with Negative Sentiments\", loc = \"center\", fontsize = 15, color = \"blue\", pad = 25)\n",
        "plt.xlabel(\"Total Counts\", color = \"magenta\", fontsize = 10, labelpad = 15)\n",
        "plt.xticks(rotation = 0)\n",
        "plt.ylabel(\"Top Words\", color = \"cyan\", fontsize = 10, labelpad = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjDMjIZaliLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that the trigrams are slightly better to describe each sentiments, although negative trigrams say a lot about bad products which we can infer from the top words above. From the N-Gram Analysis, we can also see how the decision of not removing not in our list of stopwords affects our data as we keep the meaning of negation phrases."
      ],
      "metadata": {
        "id": "ikfsXcnslmQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Clouds\n",
        "Word Cloud of Reviews with Positive Sentiments"
      ],
      "metadata": {
        "id": "FNbrxsbTlooF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "LBRd6-bdl87L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define stop words as a list\n",
        "Stopwords = ['an', 'if', 'for', 'amongst', 'seeming', 'themselves', 'before', 'about', 'around', 'name',\n",
        "             'their', 'nine', 'last', 'everyone', 'thin', 'that', 'on', 'except', 'his', 'off', 'become',\n",
        "             'is', 'wherein', 'along', 'cannot', 'nothing', 'thereafter', 'etc', 'own', 'to', 'five', 'its',\n",
        "             'the', 'ever', 'interest', 'such', 'none', 'get', 'them', 'down', 'found', 'onto', 'not', 'me',\n",
        "             'both', 'almost', 'my', 'others', 'alone', 'twenty', 'between', 'somewhere', 'top', 'former',\n",
        "             'anyhow', 'no', 'most', 'bottom', 'becoming', 'whom', 'through', 'into', 'anyone', 'been', 'we',\n",
        "             'will', 'everywhere', 'whereas', 'indeed', 'but', 'again', 'behind', 'seems', 'forty', 'find',\n",
        "             'whether', 'at', 'any', 'mine', 'may', 'latter', 'so', 'below', 'describe', 'un', 'hereupon',\n",
        "             'anyway', 'have', 'enough', 'four', 'up', 'someone', 'noone', 'whereupon', 'her', 'had',\n",
        "             'wherever', 'another', 'towards', 'already', 'as', 'therein', 'yours', 'thus', 'keep', 'eleven',\n",
        "             'many', 'several', 'either', 'six', 'do', 'cry', 'during', 'now', 'what', 'whenever', 'toward',\n",
        "             'ltd', 'meanwhile', 'yourself', 'all', 'over', 'in', 'first', 'this', 'else', 'without', 'fifteen',\n",
        "             'you', 'being', 'go', 'thick', 'herein', 'well', 'whence', 'once', 'might', 'beforehand', 'mill',\n",
        "             'more', 'somehow', 'too', 'inc', 'because', 'itself', 'formerly', 'from', 'elsewhere', 'take', 'a']\n",
        "\n",
        "# Generate word cloud\n",
        "wordCloud = WordCloud(max_words=50, width=3000, height=1500, stopwords=Stopwords).generate(str(Positive[\"reviews\"]))\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yn-uDB_VmE78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Cloud of Reviews with Neutral Sentiments"
      ],
      "metadata": {
        "id": "b3ODO2UMmaeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wordCloud = WordCloud(max_words = 50, width = 3000, height = 1500, stopwords = Stopwords).generate(str(Neutral[\"reviews\"]))\n",
        "plt.figure(figsize = (15, 15))\n",
        "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZmQB0UYYmcWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Cloud of Reviews with Negative Sentiments"
      ],
      "metadata": {
        "id": "uKzd5y97me7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wordCloud = WordCloud(max_words = 50, width = 3000, height = 1500, stopwords = Stopwords).generate(str(Negative[\"reviews\"]))\n",
        "plt.figure(figsize = (15, 15))\n",
        "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x5tWvK3imh2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these word clouds, not only we can see words that really describe our sentiments, but just like our N-Grams Analysis we can see objects being discussed in the reviews given."
      ],
      "metadata": {
        "id": "fxwP5q5tmm2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n",
        "Drop Insignificant Columns"
      ],
      "metadata": {
        "id": "pegTgpaPmqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Columns = [\"reviewerID\", \"asin\", \"reviewerName\", \"helpful\", \"unixReviewTime\", \"reviewTime\", \"polarity\", \"length\", \"word_counts\", \"overall\"]\n",
        "dataset.drop(columns = Columns, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "y0yqY1okmtmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dropped these columns to make our dataset concise. We now have two columns as our independent variables and the last column as dependent variables. To continue, we must encode our label as a set of numbers corresponding to each categories of it."
      ],
      "metadata": {
        "id": "wgxQvJWemxNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current State of The Dataset"
      ],
      "metadata": {
        "id": "eaba32w3mzY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset.head()\n",
        ""
      ],
      "metadata": {
        "id": "Fe-tQttCm1Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Our Target Variable"
      ],
      "metadata": {
        "id": "vKEKvIE2m5V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Encoder = LabelEncoder()\n",
        "dataset[\"sentiment\"] = Encoder.fit_transform(dataset[\"sentiment\"])"
      ],
      "metadata": {
        "id": "3udf-FHWm7gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "EmrVB_ncm-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We had successfully encoded our sentiment into numbers so that our model can easily figure it out. From above, we know that the label Positive is encoded into 2, Neutral into 1, and Negative into 0. Now, we have to give importance of each words in the whole review, i.e. giving them weights. We can do this by using TF-IDF (Term Frequency - Inverse Document Frequency) Vectorizer."
      ],
      "metadata": {
        "id": "x63JnGzxnBAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "ggSe0iNenDLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining our vectorizer with total words of 5000 and with bigram model\n",
        "TF_IDF = TfidfVectorizer(max_features = 5000, ngram_range = (2, 2))\n",
        "\n",
        "# Fitting and transforming our reviews into a matrix of weighed words\n",
        "# This will be our independent features\n",
        "X = TF_IDF.fit_transform(dataset[\"reviews\"])\n",
        "\n",
        "# Check our matrix shape\n",
        "X.shape"
      ],
      "metadata": {
        "id": "pue6CE_TnFpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Declaring our target variable\n",
        "y = dataset[\"sentiment\"]"
      ],
      "metadata": {
        "id": "l4Q_YzXGnH-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the shape, we successfully transformed our reviews with TF-IDF Vectorizer of 7000 top bigram words. Now, as we know from before, our data is kind of imbalanced with very little neutral and negative values compared to positive sentiments. We need to balance our dataset before going into modelling process.\n",
        "\n",
        "Resampling Our Dataset\n",
        "\n",
        "There are many ways to do resampling to an imbalanced dataset, such as SMOTE and Bootstrap Method. We will use SMOTE (Synthetic Minority Oversampling Technique) that will randomly generate new replicates of our undersampling data to balance our dataset."
      ],
      "metadata": {
        "id": "nNoaSTqrnLxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "0JkGKmbGnmxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Balancer = SMOTE(random_state = 42)\n",
        "X_final, y_final = Balancer.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "ml7zORUjoX3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Counter(y_final)"
      ],
      "metadata": {
        "id": "T_gv71EVoa2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is already balanced as we can see from the counter of each sentiment categories before and after the resampling with SMOTE."
      ],
      "metadata": {
        "id": "Shv4hRw0oeuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Our Dataset"
      ],
      "metadata": {
        "id": "AEKWm4IpogOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We splitted our dataset into 75:25 portion respectively for the training and test set."
      ],
      "metadata": {
        "id": "bCV8junqonAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.25, random_state = 42)"
      ],
      "metadata": {
        "id": "xPuDzitfoa5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection and Evaluation\n",
        "We do not really know what is the best model that fits our data well. Because of that, we will need to try every classification models available and find the best models using the Confusion Matrix and F1 Score as our main metrics, and the rest of the metrics as our support. First, we should do some cross validation techniques in order to find the best model.\n",
        "\n",
        "Model Building\n",
        "\n",
        "We are using K-Fold Cross Validation on our early dataset (before resampling) because the CV itself is not affected by the imbalanced dataset as it splits the dataset and takes into account every validations. If we use the CV on the balanced dataset that we got from resampling we should be able to get similar result."
      ],
      "metadata": {
        "id": "fCunEA2Yori7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DTree = DecisionTreeClassifier()\n",
        "LogReg = LogisticRegression()\n",
        "SVC = SVC()\n",
        "RForest = RandomForestClassifier()\n",
        "Bayes = BernoulliNB()\n",
        "KNN = KNeighborsClassifier()\n",
        "\n",
        "Models = [DTree, LogReg, SVC, RForest, Bayes, KNN]\n",
        "Models_Dict = {0: \"Decision Tree\", 1: \"Logistic Regression\", 2: \"SVC\", 3: \"Random Forest\", 4: \"Naive Bayes\", 5: \"K-Neighbors\"}\n",
        "\n",
        "for i, model in enumerate(Models):\n",
        "  print(\"{} Test Accuracy: {}\".format(Models_Dict[i], cross_val_score(model, X, y, cv = 10, scoring = \"accuracy\").mean()))"
      ],
      "metadata": {
        "id": "eINngnWloujQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got six models on our sleeves and from the results of 10-Fold Cross Validation, we know that the Logistic Regression model is the best model with the highest accuracy, slightly beating the SVC. Because of this, we will use the best model in predicting our sentiment, also to tune our parameter and evaluate the end-result of how well the model works."
      ],
      "metadata": {
        "id": "qKCtushYo0D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "qMx9pnDho2Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Param = {\"C\": np.logspace(-4, 4, 50), \"penalty\": ['l1', 'l2']}\n",
        "grid_search = GridSearchCV(estimator = LogisticRegression(random_state = 42), param_grid = Param, scoring = \"accuracy\", cv = 10, verbose = 0, n_jobs = -1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
        "print(\"Best Parameters:\", best_parameters)"
      ],
      "metadata": {
        "id": "jsKkJ9uAo5NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got a nice accuracy on our training set, which is 94.80% and from our Grid Search, we are also able to find our optimal hyperparameters. It is time to finish our model using these parameters to get the best model of Logistic Regression."
      ],
      "metadata": {
        "id": "QOVhJreco8v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Best Model"
      ],
      "metadata": {
        "id": "yt_wfdpwo_E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classifier = LogisticRegression(random_state = 42, C = 6866.488450042998, penalty = 'l2')\n",
        "Classifier.fit(X_train, y_train)\n",
        "\n",
        "Prediction = Classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "uC6rw_plpDyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is done, we will test our model on our test set. The metrics that we will evaluate is based on this prediction that we made here."
      ],
      "metadata": {
        "id": "3yV4dKfApGKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics\n",
        "\n",
        "Accuracy On Test Set"
      ],
      "metadata": {
        "id": "x4GtVCARpJJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, Prediction)"
      ],
      "metadata": {
        "id": "bF2cYheWpLxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Really high accuracy that we got here, 95.21%. Still, we need to look out for the Confusion Matrix and F1 Score to find out about our model performance."
      ],
      "metadata": {
        "id": "qCeo-aQopPop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix"
      ],
      "metadata": {
        "id": "XaZmsLs2pTz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrix = confusion_matrix(y_test, Prediction)"
      ],
      "metadata": {
        "id": "PvKggo9gpWrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing Our Confusion Matrix"
      ],
      "metadata": {
        "id": "kWZe9P8jpY-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting Function for Confusion Matrix\n",
        "def plot_cm(cm, classes, title, normalized = False, cmap = plt.cm.Blues):\n",
        "\n",
        "  plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
        "  plt.title(title, pad = 20)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  if normalized:\n",
        "    cm = cm.astype('float') / cm.sum(axis = 1)[: np.newaxis]\n",
        "    print(\"Normalized Confusion Matrix\")\n",
        "  else:\n",
        "    print(\"Unnormalized Confusion Matrix\")\n",
        "\n",
        "  threshold = cm.max() / 2\n",
        "  for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "      plt.text(j, i, cm[i, j], horizontalalignment = \"center\", color = \"white\" if cm[i, j] > threshold else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.xlabel(\"Predicted Label\", labelpad = 20)\n",
        "  plt.ylabel(\"Real Label\", labelpad = 20)"
      ],
      "metadata": {
        "id": "wrBZVrDtpdko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(ConfusionMatrix, classes = [\"Positive\", \"Neutral\", \"Negative\"], title = \"Confusion Matrix of Sentiment Analysis\")\n",
        ""
      ],
      "metadata": {
        "id": "AnAVcVTXpf5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we can gain from the Confusion Matrix above is that the model overall works well. It is able to categorize both positive and neutral sentiments correctly, while it seems to struggle a bit at determining negative sentiments. Of course, this is the effect of imbalanced data that we got from our original dataset, and luckily we can minimize the effect thanks to our SMOTE resampling before."
      ],
      "metadata": {
        "id": "jJHER4zGpisB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classification Scores\n",
        "\n",
        "\n",
        "print(classification_report(y_test, Prediction))"
      ],
      "metadata": {
        "id": "LJxayAdSpmPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, to each of our sentiment categories, we got F1 Score of 95%, which is great and because of that we can conclude that our model works well on the dataset."
      ],
      "metadata": {
        "id": "6Ocvqg39poHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "Dataset\n",
        "\n",
        "Our dataset contains many features about user reviews on musical instruments. But, we rarely need those features as our model variables because those features are not really important for sentiment analysis.\n",
        "We might need to omit our part of removing stopwords in our preprocessing phase, because there might be some important words in determining user sentiments in our model.\n",
        "From our text analysis, we know that most of the transactions made are related to guitars or other string-based instruments. We can say that guitar got a really high attention from the customers' pool and the sellers can emphasize their products on this instruments.\n",
        "Model\n",
        "\n",
        "We tried almost all classification models available. By using 10-Fold Cross Validation, we get that Logistic Regression Model got the best accuracy and we decided to use this model and tune it.\n",
        "On our attempt on making prediction to our test set, we also received a nice accuracy and high F1 Score. This means that our model works well on sentiment analysis.\n",
        "We need to consider more Cross Validation Method, such as Stratified K-Fold so that we do not really need to do resampling on our dataset. Also, we are fine without data scaling, but it is highly suggested to do it."
      ],
      "metadata": {
        "id": "fdXLemT1prMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources of Learning\n",
        "These articles and notebooks are great and really useful for sentiment analysis and NLP. Check it out!\n",
        "\n",
        "Text Preprocessing in Python: Steps, Tools, and Examples\n",
        "Sentiment Analysis — ML project from Scratch to Production (Web Application)\n",
        "Updated Text Preprocessing techniques for Sentiment Analysis\n",
        "Amazon Instrument: Sentimental Analysis\n",
        "Sentiment Analysis | Amazon reviews\n",
        "SMOTE for Imbalanced Classification with Python\n",
        "Other Documentations:\n",
        "\n",
        "Pandas\n",
        "Matplotlib\n",
        "Scikit-Learn\n",
        "Natural Language Toolkit"
      ],
      "metadata": {
        "id": "KWKusI3ppuQg"
      }
    }
  ]
}